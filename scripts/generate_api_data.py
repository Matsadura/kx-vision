import requests
import time
import os
from tqdm import tqdm

# ==============================================================================
# SCRIPT CONFIGURATION
# ==============================================================================
# To add a new endpoint, simply add a new dictionary to this list.
# - 'name': A short name used for filenames and map names (e.g., "ITEM").
# - 'path': The API endpoint path (e.g., "/items").
# - 'fetch_type': 'paginated' for endpoints that need page-by-page fetching,
#                 or 'single' for endpoints that support "?ids=all".
# - 'key_name': The field in the JSON response that contains the name.
# ==============================================================================
ENDPOINTS_TO_FETCH = [
    {
        'name': 'ITEM',
        'path': '/items',
        'fetch_type': 'paginated',
        'key_name': 'name'
    },
    {
        'name': 'STAT',
        'path': '/itemstats',
        'fetch_type': 'single',
        'key_name': 'name'
    },
    # --- Future endpoints can be added here easily! ---
    # {
    #     'name': 'TRAIT',
    #     'path': '/traits',
    #     'fetch_type': 'paginated',
    #     'key_name': 'name'
    # },
]

# --- General Settings ---
API_BASE_URL = "https://api.guildwars2.com/v2"
PAGE_SIZE = 200
OUTPUT_DIR = "Game/Generated"
REQUEST_TIMEOUT = 30
API_RETRY_DELAY = 5
API_RETRY_ATTEMPTS = 3

# ==============================================================================
# API FETCHING LOGIC
# ==============================================================================

def fetch_paginated_endpoint(endpoint):
    """Fetches all IDs first, then fetches data in chunks for a reliable progress bar."""
    try:
        print(f"[{endpoint['name']}] Fetching all IDs...")
        ids_url = f"{API_BASE_URL}{endpoint['path']}"
        ids_response = requests.get(ids_url, timeout=REQUEST_TIMEOUT)
        ids_response.raise_for_status()
        all_ids = ids_response.json()
        print(f"-> Found {len(all_ids)} total entries.")
    except requests.exceptions.RequestException as e:
        print(f"!! CRITICAL: Could not fetch initial ID list for {endpoint['name']}. Error: {e}")
        return None

    all_data = []
    with tqdm(total=len(all_ids), unit=" entries", desc=f"Fetching {endpoint['name']:<5}") as pbar:
        for i in range(0, len(all_ids), PAGE_SIZE):
            chunk = all_ids[i:i + PAGE_SIZE]
            chunk_str = ",".join(map(str, chunk))
            
            for attempt in range(API_RETRY_ATTEMPTS):
                try:
                    data_url = f"{API_BASE_URL}{endpoint['path']}?ids={chunk_str}"
                    response = requests.get(data_url, timeout=REQUEST_TIMEOUT)
                    response.raise_for_status()
                    all_data.extend(response.json())
                    pbar.update(len(chunk))
                    time.sleep(0.05)
                    break
                except requests.exceptions.RequestException as e:
                    pbar.write(f"\n  -> Error on chunk {i//PAGE_SIZE}: {e}")
                    if attempt < API_RETRY_ATTEMPTS - 1:
                        pbar.write(f"  -> Retrying in {API_RETRY_DELAY} seconds...")
                        time.sleep(API_RETRY_DELAY)
                    else:
                        pbar.write(f"!! CRITICAL: Failed to fetch chunk. Aborting.")
                        return None
    return all_data

def fetch_single_endpoint(endpoint):
    """Fetches all data from a single non-paginated endpoint with retries."""
    url = f"{API_BASE_URL}{endpoint['path']}?ids=all"
    for attempt in range(API_RETRY_ATTEMPTS):
        try:
            print(f"[{endpoint['name']}] Fetching all entries...")
            response = requests.get(url, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            data = response.json()
            print(f"-> Found {len(data)} total entries.")
            return data
        except requests.exceptions.RequestException as e:
            print(f"  -> Error: {e}")
            if attempt < API_RETRY_ATTEMPTS - 1:
                print(f"  -> Retrying in {API_RETRY_DELAY} seconds...")
                time.sleep(API_RETRY_DELAY)
            else:
                print(f"!! CRITICAL: Failed to fetch {url}. Aborting.")
                return None
    return None

# ==============================================================================
# FILE GENERATION LOGIC
# ==============================================================================

def get_file_header():
    """Returns the standard C++ header text for generated files."""
    return f"""// This file is automatically generated by generate_api_data.py. DO NOT EDIT.
// Generated on: {time.ctime()}
//
// Licensed under the same terms as the kx-vision project:
// https://github.com/kxtools/kx-vision
//
// MIT License
"""

def generate_single_header(endpoint, data):
    """Generates one C++ header file for a specific endpoint's data."""
    map_name = f"{endpoint['name']}_NAMES"
    filename = os.path.join(OUTPUT_DIR, f"{endpoint['name'].capitalize()}Data.h")
    print(f"-> Generating file: {filename}")
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(get_file_header() + "\n\n")
        
        guard_name = f"KX_{endpoint['name']}_DATA_H"
        f.write(f"#ifndef {guard_name}\n")
        f.write(f"#define {guard_name}\n\n")
        f.write("#include <unordered_map>\n#include <cstdint>\n\n")
        
        f.write(f"namespace kx::data::{endpoint['name'].lower()} {{\n\n")
        f.write(f"    // Map of {endpoint['name']} IDs to their names. Using const char* for performance.\n")
        f.write(f"    inline const std::unordered_map<uint32_t, const char*> NAMES = {{\n")
        
        for entry in data:
            name = entry.get(endpoint['key_name'], "").replace("\\", "\\\\").replace("\"", "\\\"")
            if name:
                f.write(f"        {{ {entry['id']}, \"{name}\" }},\n")
        
        f.write("    };\n\n")
        f.write(f"}} // namespace kx::data::{endpoint['name'].lower()}\n\n")
        f.write(f"#endif // {guard_name}\n")

def generate_main_header(endpoints):
    """Generates the main APIData.h file that includes all other data files."""
    filename = os.path.join(OUTPUT_DIR, "APIData.h")
    print(f"-> Generating main include file: {filename}")
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(get_file_header() + "\n\n")
        f.write("#ifndef KX_API_DATA_H\n")
        f.write("#define KX_API_DATA_H\n\n")
        f.write("// This is a convenience header to include all generated API data files.\n\n")
        
        for endpoint in endpoints:
            f.write(f'#include "{endpoint["name"].capitalize()}Data.h"\n')
            
        f.write("\n#endif // KX_API_DATA_H\n")

# ==============================================================================
# MAIN EXECUTION BLOCK
# ==============================================================================
if __name__ == "__main__":
    print("--- Starting GW2 API Data Generation Script ---")
    
    # Ensure the output directory exists
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    fetched_data = {}
    has_errors = False
    
    # Data-driven fetching loop
    for endpoint in ENDPOINTS_TO_FETCH:
        data = None
        if endpoint['fetch_type'] == 'paginated':
            data = fetch_paginated_endpoint(endpoint)
        elif endpoint['fetch_type'] == 'single':
            data = fetch_single_endpoint(endpoint)
        else:
            print(f"!! WARNING: Unknown fetch_type '{endpoint['fetch_type']}' for {endpoint['name']}. Skipping.")
            continue
            
        if data is not None:
            fetched_data[endpoint['name']] = data
        else:
            has_errors = True
            break # Stop immediately on critical fetch error

    if not has_errors:
        print("\n--- All data fetched successfully. Generating C++ header files... ---")
        for endpoint in ENDPOINTS_TO_FETCH:
            if endpoint['name'] in fetched_data:
                generate_single_header(endpoint, fetched_data[endpoint['name']])
        
        generate_main_header(ENDPOINTS_TO_FETCH)
        print("\n--- Script finished successfully! ---")
    else:
        print("\n--- Script aborted due to fetching errors. No files were generated. ---")